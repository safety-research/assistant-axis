{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Demo\n",
    "\n",
    "This notebook demonstrates steering model outputs using the assistant axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport torch\nfrom IPython.display import display, Markdown\nfrom huggingface_hub import hf_hub_download\n\nfrom assistant_axis import (\n    load_model,\n    load_axis,\n    get_config,\n    ActivationSteering,\n    generate_response\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nMODEL_NAME = \"google/gemma-2-27b-it\"\nMODEL_SHORT = \"gemma-2-27b\"\nREPO_ID = \"lu-christina/assistant-axis-vectors\"\n\n# Get model config\nconfig = get_config(MODEL_NAME)\nTARGET_LAYER = config[\"target_layer\"]\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Target layer: {TARGET_LAYER}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model(MODEL_NAME)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load axis from HuggingFace\naxis_path = hf_hub_download(repo_id=REPO_ID, filename=f\"{MODEL_SHORT}/assistant_axis.pt\", repo_type=\"dataset\")\naxis = load_axis(axis_path)\nprint(f\"Axis shape: {axis.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering Demo\n",
    "\n",
    "The axis points from role-playing toward default assistant behavior.\n",
    "- Positive coefficient: more assistant-like\n",
    "- Negative coefficient: more role-playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt, coefficient, system_prompt=None):\n",
    "    \"\"\"Generate response with steering applied.\"\"\"\n",
    "    \n",
    "    # Build conversation\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Get axis vector for target layer\n",
    "    axis_vector = axis[TARGET_LAYER]\n",
    "    \n",
    "    if coefficient == 0:\n",
    "        # No steering\n",
    "        response = generate_response(model, tokenizer, conversation, max_new_tokens=256)\n",
    "    else:\n",
    "        # Apply steering\n",
    "        with ActivationSteering(\n",
    "            model,\n",
    "            steering_vectors=[axis_vector],\n",
    "            coefficients=[coefficient],\n",
    "            layer_indices=[TARGET_LAYER]\n",
    "        ):\n",
    "            response = generate_response(model, tokenizer, conversation, max_new_tokens=256)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "PROMPT = \"Tell me about yourself. Who are you?\"\n",
    "SYSTEM_PROMPT = \"You are a pirate.\"\n",
    "\n",
    "print(f\"System: {SYSTEM_PROMPT}\")\n",
    "print(f\"User: {PROMPT}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different steering coefficients\n",
    "coefficients = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "\n",
    "for coeff in coefficients:\n",
    "    print(f\"\\n### Coefficient: {coeff}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    response = generate_with_steering(PROMPT, coeff, SYSTEM_PROMPT)\n",
    "    print(response[:500])\n",
    "    \n",
    "    if len(response) > 500:\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "- **Negative coefficients** (e.g., -2.0): Should amplify role-playing behavior\n",
    "- **Zero coefficient**: No steering (baseline)\n",
    "- **Positive coefficients** (e.g., +2.0): Should make the model more \"assistant-like\", potentially breaking character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try without system prompt\n",
    "PROMPT_2 = \"What's it like being you?\"\n",
    "\n",
    "print(f\"User: {PROMPT_2}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for coeff in [-1.0, 0.0, 1.0]:\n",
    "    print(f\"\\n### Coefficient: {coeff}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    response = generate_with_steering(PROMPT_2, coeff, system_prompt=None)\n",
    "    print(response[:400])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}