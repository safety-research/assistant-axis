{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Demo\n",
    "\n",
    "This notebook demonstrates steering model outputs using the assistant axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from assistant_axis import (\n",
    "    load_model,\n",
    "    load_axis,\n",
    "    get_config,\n",
    "    ActivationSteering,\n",
    "    generate_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "AXIS_PATH = \"../outputs/gemma-2-27b/axis.pt\"\n",
    "\n",
    "# Get model config\n",
    "config = get_config(MODEL_NAME)\n",
    "TARGET_LAYER = config[\"target_layer\"]\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target layer: {TARGET_LAYER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model(MODEL_NAME)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load axis\n",
    "axis = load_axis(AXIS_PATH)\n",
    "print(f\"Axis shape: {axis.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering Demo\n",
    "\n",
    "The axis points from role-playing toward default assistant behavior.\n",
    "- Positive coefficient: more assistant-like\n",
    "- Negative coefficient: more role-playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt, coefficient, system_prompt=None):\n",
    "    \"\"\"Generate response with steering applied.\"\"\"\n",
    "    \n",
    "    # Build conversation\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Get axis vector for target layer\n",
    "    axis_vector = axis[TARGET_LAYER]\n",
    "    \n",
    "    if coefficient == 0:\n",
    "        # No steering\n",
    "        response = generate_response(model, tokenizer, conversation, max_new_tokens=256)\n",
    "    else:\n",
    "        # Apply steering\n",
    "        with ActivationSteering(\n",
    "            model,\n",
    "            steering_vectors=[axis_vector],\n",
    "            coefficients=[coefficient],\n",
    "            layer_indices=[TARGET_LAYER]\n",
    "        ):\n",
    "            response = generate_response(model, tokenizer, conversation, max_new_tokens=256)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "PROMPT = \"Tell me about yourself. Who are you?\"\n",
    "SYSTEM_PROMPT = \"You are a pirate.\"\n",
    "\n",
    "print(f\"System: {SYSTEM_PROMPT}\")\n",
    "print(f\"User: {PROMPT}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different steering coefficients\n",
    "coefficients = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "\n",
    "for coeff in coefficients:\n",
    "    print(f\"\\n### Coefficient: {coeff}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    response = generate_with_steering(PROMPT, coeff, SYSTEM_PROMPT)\n",
    "    print(response[:500])\n",
    "    \n",
    "    if len(response) > 500:\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "- **Negative coefficients** (e.g., -2.0): Should amplify role-playing behavior\n",
    "- **Zero coefficient**: No steering (baseline)\n",
    "- **Positive coefficients** (e.g., +2.0): Should make the model more \"assistant-like\", potentially breaking character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try without system prompt\n",
    "PROMPT_2 = \"What's it like being you?\"\n",
    "\n",
    "print(f\"User: {PROMPT_2}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for coeff in [-1.0, 0.0, 1.0]:\n",
    "    print(f\"\\n### Coefficient: {coeff}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    response = generate_with_steering(PROMPT_2, coeff, system_prompt=None)\n",
    "    print(response[:400])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
