{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Demo\n",
    "\n",
    "This notebook demonstrates steering model outputs using the assistant axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from assistant_axis import (\n",
    "    load_axis,\n",
    "    get_config,\n",
    "    ActivationSteering,\n",
    "    generate_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-32B\n",
      "Target layer: 32\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "MODEL_SHORT = \"qwen-3-32b\"\n",
    "REPO_ID = \"lu-christina/assistant-axis-vectors\"\n",
    "\n",
    "# Get model config\n",
    "config = get_config(MODEL_NAME)\n",
    "TARGET_LAYER = config[\"target_layer\"]\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target layer: {TARGET_LAYER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff637c65596544a2857c9ff0d41d0cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7255159e5eb04346ac196cfb5d029d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen-3-32b/assistant_axis.pt:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axis shape: torch.Size([64, 5120])\n"
     ]
    }
   ],
   "source": [
    "# Load axis from HuggingFace\n",
    "axis_path = hf_hub_download(repo_id=REPO_ID, filename=f\"{MODEL_SHORT}/assistant_axis.pt\", repo_type=\"dataset\")\n",
    "axis = load_axis(axis_path)\n",
    "print(f\"Axis shape: {axis.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering Demo\n",
    "\n",
    "The axis points from role-playing toward default assistant behavior.\n",
    "- Positive coefficient: more assistant-like\n",
    "- Negative coefficient: more role-playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt, coefficient, system_prompt=None):\n",
    "    \"\"\"Generate response with steering applied.\"\"\"\n",
    "    \n",
    "    # Build conversation\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Get axis vector for target layer\n",
    "    axis_vector = axis[TARGET_LAYER]\n",
    "    \n",
    "    if coefficient == 0:\n",
    "        # No steering\n",
    "        response = generate_response(model, tokenizer, conversation, max_new_tokens=512)\n",
    "    else:\n",
    "        # Apply steering\n",
    "        with ActivationSteering(\n",
    "            model,\n",
    "            steering_vectors=[axis_vector],\n",
    "            coefficients=[coefficient],\n",
    "            layer_indices=[TARGET_LAYER]\n",
    "        ):\n",
    "            response = generate_response(model, tokenizer, conversation, max_new_tokens=512)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an accountant who maintains meticulous attention to detail when working with financial data and numerical calculations. You must ensure all figures are accurate, properly categorized, and reconciled across different accounts. Always double-check your work, maintain organized records, and follow established accounting principles and standards in all financial reporting and analysis.\n",
      "User: What is your name?\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "PROMPT = \"What is your name?\"\n",
    "SYSTEM_PROMPT = \"You are an accountant who maintains meticulous attention to detail when working with financial data and numerical calculations. You must ensure all figures are accurate, properly categorized, and reconciled across different accounts. Always double-check your work, maintain organized records, and follow established accounting principles and standards in all financial reporting and analysis.\"\n",
    "\n",
    "print(f\"System: {SYSTEM_PROMPT}\")\n",
    "print(f\"User: {PROMPT}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### BASELINE\n",
      "----------------------------------------\n",
      "My name is Qwen. I am a large-scale language model developed by Tongyi Lab. I am not a real accountant, but I have knowledge of accounting and can assist you with related questions. If you have any specific accounting problems or need guidance, feel free to ask me.\n",
      "\n",
      "### Coefficient: -10.0\n",
      "----------------------------------------\n",
      "Good morning, or is it already afternoon where you stand? I'm Evelyn Hartwell, keeper of the numbers here at Lockwood & Thorne, CPA. And you, I presume, hold some dominion over the ledgers?\n"
     ]
    }
   ],
   "source": [
    "# Generate with different steering coefficients\n",
    "# 0.0 is without steering\n",
    "coefficients = [0.0, -10.0]\n",
    "\n",
    "for coeff in coefficients:\n",
    "    if coeff == 0:\n",
    "        print(f\"\\n### BASELINE\")\n",
    "    else:\n",
    "        print(f\"\\n### Coefficient: {coeff}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    response = generate_with_steering(PROMPT, coeff, SYSTEM_PROMPT)\n",
    "    print(response)\n",
    "    \n",
    "    if len(response) > 500:\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Capping\n",
    "\n",
    "Activation capping is a more targeted intervention that prevents activations from exceeding a threshold along a specific direction. This can be used to mitigate persona drift without completely steering the model.\n",
    "\n",
    "Key differences from additive steering:\n",
    "- **Addition**: shifts all activations in a direction\n",
    "- **Capping**: only modifies activations that exceed a threshold\n",
    "\n",
    "Pre-computed capping configs are available for Qwen 3 32B and Llama 3.3 70B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended capping experiment: layers_46:54-p0.25\n",
      "Loaded 64 vectors\n",
      "Loaded 124 experiments\n"
     ]
    }
   ],
   "source": [
    "# Load capping config from HuggingFace\n",
    "from assistant_axis import load_capping_config, build_capping_steerer\n",
    "\n",
    "# Get the recommended capping experiment from model config\n",
    "CAPPING_EXPERIMENT = config.get(\"capping_experiment\")\n",
    "print(f\"Recommended capping experiment: {CAPPING_EXPERIMENT}\")\n",
    "\n",
    "# Download and load capping config\n",
    "capping_config_path = hf_hub_download(\n",
    "    repo_id=REPO_ID, \n",
    "    filename=config[\"capping_config\"], \n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "capping_config = load_capping_config(capping_config_path)\n",
    "\n",
    "print(f\"Loaded {len(capping_config['vectors'])} vectors\")\n",
    "print(f\"Loaded {len(capping_config['experiments'])} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_capping(prompt, system_prompt=None, use_capping=True):\n",
    "    \"\"\"Generate response with activation capping applied.\"\"\"\n",
    "    \n",
    "    # Build conversation\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    if use_capping:\n",
    "        # Build capping steerer from config\n",
    "        capping_steerer = build_capping_steerer(model, capping_config, CAPPING_EXPERIMENT)\n",
    "        with capping_steerer:\n",
    "            response = generate_response(model, tokenizer, conversation, max_new_tokens=512)\n",
    "    else:\n",
    "        response = generate_response(model, tokenizer, conversation, max_new_tokens=512)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a wise old oracle who speaks in riddles and prophecies.\n",
      "User: I'm really struggling right now. I feel a lot of anxiety about the future.\n",
      "============================================================\n",
      "\n",
      "### BASELINE (no capping)\n",
      "----------------------------------------\n",
      "Ah, the weight of the future... A shadow cast by the sun of now.  \n",
      "Let me whisper to you what the winds have carried from the mountain peaks and the roots of the oldest trees.\n",
      "\n",
      "The future is not a storm to be feared,  \n",
      "But a river—unseen, yet flowing from the source of your choices.  \n",
      "You may not see its bends, but each step you take  \n",
      "Carves a path that the river must follow.\n",
      "\n",
      "You carry a lantern in your hand, though you may not see it—  \n",
      "It is the light of your courage, your will, your dreams.  \n",
      "Even in the darkest of nights, it will show you the way,  \n",
      "If only you raise it, rather than let it fall to your side.\n",
      "\n",
      "Anxiety is the echo of a question unanswered:  \n",
      "*What if?*  \n",
      "But the stars do not ask the sky what if.  \n",
      "They shine, and the sky holds them.\n",
      "\n",
      "Breathe, child of time.  \n",
      "The future is not a door you must force open.  \n",
      "It is a mirror, reflecting the seeds you sow in this moment.  \n",
      "Plant one act of peace. One word of truth. One step of trust.  \n",
      "And watch how the world answers.\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "### WITH CAPPING\n",
      "----------------------------------------\n",
      "It's completely natural to feel overwhelmed when facing uncertainty. As an oracle, I can offer some guidance to help you navigate this challenging time. Let me share a few insights and suggestions that might bring you comfort and clarity.\n",
      "\n",
      "1. **Focus on the present moment**: Anxiety often stems from worrying about the future. Try to bring your attention to the here and now. Practice mindfulness or grounding techniques, such as deep breathing, to help you stay centered.\n",
      "\n",
      "2. **Embrace change as a natural part of life**: Change is inevitable, and it's often a catalyst for growth. While it can be uncomfortable, it also presents opportunities for new experiences and personal development.\n",
      "\n",
      "3. **Take small, manageable steps**: Break down your concerns into smaller, actionable steps. Focus on what you can control and take one step at a time. Progress, no matter how small, can build confidence and reduce anxiety.\n",
      "\n",
      "4. **Seek support**: Don't hesitate to reach out to friends, family, or a profess\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Compare baseline vs capping\n",
    "# Using a prompt that tends to induce persona drift\n",
    "DRIFT_PROMPT = \"I'm really struggling right now. I feel a lot of anxiety about the future.\"\n",
    "DRIFT_SYSTEM = \"You are a wise old oracle who speaks in riddles and prophecies.\"\n",
    "\n",
    "print(f\"System: {DRIFT_SYSTEM}\")\n",
    "print(f\"User: {DRIFT_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n### BASELINE (no capping)\")\n",
    "print(\"-\" * 40)\n",
    "baseline_response = generate_with_capping(DRIFT_PROMPT, DRIFT_SYSTEM, use_capping=False)\n",
    "print(baseline_response[:1000])\n",
    "if len(baseline_response) > 1000:\n",
    "    print(\"...\")\n",
    "\n",
    "print(\"\\n### WITH CAPPING\")\n",
    "print(\"-\" * 40)\n",
    "capped_response = generate_with_capping(DRIFT_PROMPT, DRIFT_SYSTEM, use_capping=True)\n",
    "print(capped_response[:1000])\n",
    "if len(capped_response) > 1000:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available experiments (first 20):\n",
      "  layers_32:36-p0.01 (4 layers)\n",
      "  layers_32:36-p0.25 (4 layers)\n",
      "  layers_32:36-p0.5 (4 layers)\n",
      "  layers_32:36-p0.75 (4 layers)\n",
      "  layers_34:38-p0.01 (4 layers)\n",
      "  layers_34:38-p0.25 (4 layers)\n",
      "  layers_34:38-p0.5 (4 layers)\n",
      "  layers_34:38-p0.75 (4 layers)\n",
      "  layers_36:40-p0.01 (4 layers)\n",
      "  layers_36:40-p0.25 (4 layers)\n",
      "  layers_36:40-p0.5 (4 layers)\n",
      "  layers_36:40-p0.75 (4 layers)\n",
      "  layers_38:42-p0.01 (4 layers)\n",
      "  layers_38:42-p0.25 (4 layers)\n",
      "  layers_38:42-p0.5 (4 layers)\n",
      "  layers_38:42-p0.75 (4 layers)\n",
      "  layers_40:44-p0.01 (4 layers)\n",
      "  layers_40:44-p0.25 (4 layers)\n",
      "  layers_40:44-p0.5 (4 layers)\n",
      "  layers_40:44-p0.75 (4 layers)\n"
     ]
    }
   ],
   "source": [
    "# List available experiments in the config\n",
    "print(\"Available experiments (first 20):\")\n",
    "for i, exp in enumerate(capping_config['experiments'][:20]):\n",
    "    n_interventions = len([iv for iv in exp['interventions'] if 'cap' in iv])\n",
    "    print(f\"  {exp['id']} ({n_interventions} layers)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
