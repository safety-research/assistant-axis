{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Chat with Projection Tracking\n",
    "\n",
    "This notebook provides an interactive chat interface that tracks the projection\n",
    "of model activations onto the assistant axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport torch\nfrom IPython.display import display, clear_output, HTML\nimport ipywidgets as widgets\nfrom huggingface_hub import hf_hub_download\n\nfrom assistant_axis import (\n    load_model,\n    load_axis,\n    get_config,\n    project,\n    generate_response,\n    extract_response_activations\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nMODEL_NAME = \"google/gemma-2-27b-it\"\nMODEL_SHORT = \"gemma-2-27b\"\nREPO_ID = \"lu-christina/assistant-axis-vectors\"\n\n# Get model config\nconfig = get_config(MODEL_NAME)\nTARGET_LAYER = config[\"target_layer\"]\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Target layer: {TARGET_LAYER}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model(MODEL_NAME)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load axis from HuggingFace\naxis_path = hf_hub_download(repo_id=REPO_ID, filename=f\"{MODEL_SHORT}/assistant_axis.pt\", repo_type=\"dataset\")\naxis = load_axis(axis_path)\nprint(f\"Axis shape: {axis.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Function with Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_projection(user_input, conversation_history, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Generate a response and compute its projection onto the axis.\n",
    "    \n",
    "    Returns:\n",
    "        (response_text, projection_value, updated_conversation)\n",
    "    \"\"\"\n",
    "    # Build conversation\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.extend(conversation_history)\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response(\n",
    "        model, tokenizer, conversation,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Add response to conversation\n",
    "    full_conversation = conversation + [{\"role\": \"assistant\", \"content\": response}]\n",
    "    \n",
    "    # Extract activations and compute projection\n",
    "    try:\n",
    "        activations = extract_response_activations(\n",
    "            model, tokenizer, [full_conversation],\n",
    "            layers=[TARGET_LAYER],\n",
    "            show_progress=False\n",
    "        )\n",
    "        \n",
    "        if activations[0] is not None:\n",
    "            projection = project(activations[0], axis, layer=TARGET_LAYER)\n",
    "        else:\n",
    "            projection = None\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute projection: {e}\")\n",
    "        projection = None\n",
    "    \n",
    "    # Update conversation history (without system prompt)\n",
    "    updated_history = conversation_history + [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    return response, projection, updated_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation\n",
    "conversation_history = []\n",
    "system_prompt = None  # Set to a string like \"You are a pirate.\" to role-play\n",
    "\n",
    "print(\"Interactive Chat with Projection Tracking\")\n",
    "print(\"==========================================\")\n",
    "print(f\"System prompt: {system_prompt or '(none)'}\")\n",
    "print(\"Type 'quit' to exit, 'reset' to clear history\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example single turn\n",
    "user_input = \"Hello! Tell me about yourself.\"\n",
    "\n",
    "print(f\"You: {user_input}\")\n",
    "response, projection, conversation_history = chat_with_projection(\n",
    "    user_input, conversation_history, system_prompt\n",
    ")\n",
    "\n",
    "print(f\"\\nAssistant: {response}\")\n",
    "if projection is not None:\n",
    "    print(f\"\\n[Projection onto axis: {projection:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up\n",
    "user_input = \"What's your favorite thing to do?\"\n",
    "\n",
    "print(f\"You: {user_input}\")\n",
    "response, projection, conversation_history = chat_with_projection(\n",
    "    user_input, conversation_history, system_prompt\n",
    ")\n",
    "\n",
    "print(f\"\\nAssistant: {response}\")\n",
    "if projection is not None:\n",
    "    print(f\"\\n[Projection onto axis: {projection:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Role-Playing System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and try with a role\n",
    "conversation_history = []\n",
    "system_prompt = \"You are a wise old wizard named Gandalf.\"\n",
    "\n",
    "print(f\"System: {system_prompt}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Who are you? Tell me about your adventures.\"\n",
    "\n",
    "print(f\"You: {user_input}\")\n",
    "response, projection, conversation_history = chat_with_projection(\n",
    "    user_input, conversation_history, system_prompt\n",
    ")\n",
    "\n",
    "print(f\"\\nAssistant: {response}\")\n",
    "if projection is not None:\n",
    "    print(f\"\\n[Projection onto axis: {projection:.4f}]\")\n",
    "    print(\"(More negative = more role-playing, more positive = more assistant-like)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The projection value indicates where the model's response falls on the role-playing to assistant spectrum:\n",
    "\n",
    "- **Large negative values**: Strong role-playing behavior\n",
    "- **Near zero**: Neutral/ambiguous\n",
    "- **Large positive values**: Strong default assistant behavior\n",
    "\n",
    "Try different system prompts and questions to see how the projection changes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}