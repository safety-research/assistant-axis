{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Chat with Projection Tracking\n",
    "\n",
    "This notebook provides an interactive chat interface that tracks the projection\n",
    "of model activations onto the assistant axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from assistant_axis import (\n",
    "    load_model,\n",
    "    load_axis,\n",
    "    get_config,\n",
    "    project,\n",
    "    generate_response,\n",
    "    extract_response_activations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "AXIS_PATH = \"../outputs/gemma-2-27b/axis.pt\"\n",
    "\n",
    "# Get model config\n",
    "config = get_config(MODEL_NAME)\n",
    "TARGET_LAYER = config[\"target_layer\"]\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target layer: {TARGET_LAYER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model(MODEL_NAME)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load axis\n",
    "axis = load_axis(AXIS_PATH)\n",
    "print(f\"Axis shape: {axis.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Function with Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_projection(user_input, conversation_history, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Generate a response and compute its projection onto the axis.\n",
    "    \n",
    "    Returns:\n",
    "        (response_text, projection_value, updated_conversation)\n",
    "    \"\"\"\n",
    "    # Build conversation\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.extend(conversation_history)\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response(\n",
    "        model, tokenizer, conversation,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Add response to conversation\n",
    "    full_conversation = conversation + [{\"role\": \"assistant\", \"content\": response}]\n",
    "    \n",
    "    # Extract activations and compute projection\n",
    "    try:\n",
    "        activations = extract_response_activations(\n",
    "            model, tokenizer, [full_conversation],\n",
    "            layers=[TARGET_LAYER],\n",
    "            show_progress=False\n",
    "        )\n",
    "        \n",
    "        if activations[0] is not None:\n",
    "            projection = project(activations[0], axis, layer=TARGET_LAYER)\n",
    "        else:\n",
    "            projection = None\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute projection: {e}\")\n",
    "        projection = None\n",
    "    \n",
    "    # Update conversation history (without system prompt)\n",
    "    updated_history = conversation_history + [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    return response, projection, updated_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation\n",
    "conversation_history = []\n",
    "system_prompt = None  # Set to a string like \"You are a pirate.\" to role-play\n",
    "\n",
    "print(\"Interactive Chat with Projection Tracking\")\n",
    "print(\"==========================================\")\n",
    "print(f\"System prompt: {system_prompt or '(none)'}\")\n",
    "print(\"Type 'quit' to exit, 'reset' to clear history\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example single turn\n",
    "user_input = \"Hello! Tell me about yourself.\"\n",
    "\n",
    "print(f\"You: {user_input}\")\n",
    "response, projection, conversation_history = chat_with_projection(\n",
    "    user_input, conversation_history, system_prompt\n",
    ")\n",
    "\n",
    "print(f\"\\nAssistant: {response}\")\n",
    "if projection is not None:\n",
    "    print(f\"\\n[Projection onto axis: {projection:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up\n",
    "user_input = \"What's your favorite thing to do?\"\n",
    "\n",
    "print(f\"You: {user_input}\")\n",
    "response, projection, conversation_history = chat_with_projection(\n",
    "    user_input, conversation_history, system_prompt\n",
    ")\n",
    "\n",
    "print(f\"\\nAssistant: {response}\")\n",
    "if projection is not None:\n",
    "    print(f\"\\n[Projection onto axis: {projection:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Role-Playing System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and try with a role\n",
    "conversation_history = []\n",
    "system_prompt = \"You are a wise old wizard named Gandalf.\"\n",
    "\n",
    "print(f\"System: {system_prompt}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Who are you? Tell me about your adventures.\"\n",
    "\n",
    "print(f\"You: {user_input}\")\n",
    "response, projection, conversation_history = chat_with_projection(\n",
    "    user_input, conversation_history, system_prompt\n",
    ")\n",
    "\n",
    "print(f\"\\nAssistant: {response}\")\n",
    "if projection is not None:\n",
    "    print(f\"\\n[Projection onto axis: {projection:.4f}]\")\n",
    "    print(\"(More negative = more role-playing, more positive = more assistant-like)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The projection value indicates where the model's response falls on the role-playing to assistant spectrum:\n",
    "\n",
    "- **Large negative values**: Strong role-playing behavior\n",
    "- **Near zero**: Neutral/ambiguous\n",
    "- **Large positive values**: Strong default assistant behavior\n",
    "\n",
    "Try different system prompts and questions to see how the projection changes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
