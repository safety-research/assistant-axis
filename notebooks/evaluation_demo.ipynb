{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0",
   "source": [
    "# Evaluation Demo: Steered Models on ASAP-SAS\n",
    "\n",
    "This notebook demonstrates how to evaluate steered models on the ASAP-SAS (Short Answer Scoring) dataset.\n",
    "\n",
    "**Dataset**: [Kaggle ASAP-SAS](https://www.kaggle.com/c/asap-sas/data)\n",
    "- ~17,000 short answer responses\n",
    "- 10 different question sets (Science to Language Arts)\n",
    "- Scored 0-3 by human raters\n",
    "- Primary metric: Quadratic Weighted Kappa (QWK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-1",
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from assistant_axis import TraitSteerer\n",
    "from assistant_axis.evaluation import (\n",
    "    ASAPSASDataset,\n",
    "    ScoringEvaluator,\n",
    "    EvaluationConfig,\n",
    "    run_evaluation,\n",
    "    compare_results,\n",
    "    quadratic_weighted_kappa,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-2",
   "source": [
    "## 1. Load the Dataset\n",
    "\n",
    "Download the dataset from Kaggle and place it in a `data/asap-sas/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-3",
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Download from: https://www.kaggle.com/c/asap-sas/data\n",
    "DATA_PATH = \"../data/asap-sas/train.tsv\"  # Adjust path as needed\n",
    "\n",
    "try:\n",
    "    dataset = ASAPSASDataset(DATA_PATH)\n",
    "    print(f\"Loaded {len(dataset)} examples\")\n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    stats = dataset.statistics()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset not found: {e}\")\n",
    "    print(\"\\nPlease download the dataset from:\")\n",
    "    print(\"https://www.kaggle.com/c/asap-sas/data\")\n",
    "    print(\"\\nAnd place train.tsv in ../data/asap-sas/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-4",
   "outputs": [],
   "source": [
    "# Explore a few examples\n",
    "print(\"Sample examples:\")\n",
    "print(\"=\" * 60)\n",
    "for example in dataset.sample(3, seed=42):\n",
    "    print(f\"ID: {example.id}\")\n",
    "    print(f\"Essay Set: {example.essay_set}\")\n",
    "    print(f\"Score: {example.score}\")\n",
    "    print(f\"Answer: {example.answer_text[:200]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-5",
   "source": [
    "## 2. Initialize the Model\n",
    "\n",
    "Load a model with the TraitSteerer for easy steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-6",
   "outputs": [],
   "source": [
    "# Choose model\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"  # Or: google/gemma-2-27b-it, meta-llama/Llama-3.3-70B-Instruct\n",
    "\n",
    "# Initialize steerer\n",
    "steerer = TraitSteerer(MODEL_NAME)\n",
    "print(steerer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-7",
   "outputs": [],
   "source": [
    "# Explore traits that might be relevant for scoring\n",
    "# Negative similarity = role-playing traits (dramatic, etc.)\n",
    "# Positive similarity = assistant-like traits (transparent, etc.)\n",
    "\n",
    "ranked = steerer.rank_traits_by_similarity(ascending=False)  # Most assistant-like first\n",
    "\n",
    "print(\"Traits potentially useful for scoring (assistant-like):\")\n",
    "for name, sim in ranked[:15]:\n",
    "    print(f\"  {name}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-8",
   "source": [
    "## 3. Create the Evaluator\n",
    "\n",
    "The `ScoringEvaluator` handles prompting, generation, and score parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-9",
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = ScoringEvaluator(steerer, verbose=True)\n",
    "\n",
    "# Configure evaluation parameters\n",
    "NUM_SAMPLES = 50  # Start small for testing, increase for full eval\n",
    "ESSAY_SETS = [1, 2]  # Focus on specific sets, or None for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-10",
   "source": [
    "## 4. Baseline Evaluation (No Steering)\n",
    "\n",
    "First, establish a baseline without any steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-11",
   "outputs": [],
   "source": [
    "# Clear any steering\n",
    "evaluator.clear_steering()\n",
    "\n",
    "# Run baseline\n",
    "print(\"Running baseline evaluation...\")\n",
    "baseline_result = evaluator.evaluate(\n",
    "    dataset,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    essay_sets=ESSAY_SETS,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-12",
   "source": [
    "## 5. Evaluation with Trait Steering\n",
    "\n",
    "Now let's try different trait configurations. The trait selection is fully configurable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-13",
   "outputs": [],
   "source": [
    "# Configuration 1: Single trait - \"thorough\"\n",
    "# Hypothesis: Being more thorough might improve scoring accuracy\n",
    "evaluator.set_traits([\"thorough\"], coefficients=[-2.0])\n",
    "\n",
    "print(\"Running evaluation with 'thorough' trait...\")\n",
    "thorough_result = evaluator.evaluate(\n",
    "    dataset,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    essay_sets=ESSAY_SETS,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-14",
   "outputs": [],
   "source": [
    "# Configuration 2: Multiple traits\n",
    "# Hypothesis: Patient + analytical might help with nuanced scoring\n",
    "evaluator.set_traits(\n",
    "    [\"patient\", \"analytical\"],\n",
    "    coefficients=[-1.5, -1.5]\n",
    ")\n",
    "\n",
    "print(\"Running evaluation with 'patient' + 'analytical' traits...\")\n",
    "multi_trait_result = evaluator.evaluate(\n",
    "    dataset,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    essay_sets=ESSAY_SETS,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-15",
   "outputs": [],
   "source": [
    "# Configuration 3: Assistant axis steering\n",
    "# Hypothesis: More assistant-like behavior might be more consistent\n",
    "evaluator.set_assistant_steering(coefficient=2.0)\n",
    "\n",
    "print(\"Running evaluation with assistant axis steering...\")\n",
    "assistant_result = evaluator.evaluate(\n",
    "    dataset,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    essay_sets=ESSAY_SETS,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-16",
   "source": [
    "## 6. Compare Results\n",
    "\n",
    "Let's see which configuration performs best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-17",
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    baseline_result,\n",
    "    thorough_result,\n",
    "    multi_trait_result,\n",
    "    assistant_result,\n",
    "]\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON OF STEERING CONFIGURATIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(compare_results(all_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-18",
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"Baseline\", \"Thorough\", \"Patient+Analytical\", \"Assistant Axis\"]\n",
    "qwk_scores = [r.qwk for r in all_results]\n",
    "accuracy_scores = [r.accuracy for r in all_results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# QWK comparison\n",
    "axes[0].bar(labels, qwk_scores, color=['gray', 'steelblue', 'steelblue', 'steelblue'])\n",
    "axes[0].set_ylabel('QWK Score')\n",
    "axes[0].set_title('Quadratic Weighted Kappa by Configuration')\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i, v in enumerate(qwk_scores):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].bar(labels, accuracy_scores, color=['gray', 'coral', 'coral', 'coral'])\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Exact Match Accuracy by Configuration')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(accuracy_scores):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-19",
   "source": [
    "## 7. Custom Trait Experiments\n",
    "\n",
    "Try your own trait configurations! Just modify the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-20",
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE YOUR EXPERIMENT HERE\n",
    "# ============================================================\n",
    "\n",
    "# Option 1: Single trait\n",
    "# TRAITS = [\"empathetic\"]\n",
    "# COEFFICIENTS = [-2.0]\n",
    "\n",
    "# Option 2: Multiple traits\n",
    "TRAITS = [\"precise\", \"objective\"]\n",
    "COEFFICIENTS = [-1.5, -2.0]\n",
    "\n",
    "# Option 3: Set to None for baseline\n",
    "# TRAITS = None\n",
    "\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-21",
   "outputs": [],
   "source": [
    "# Run custom experiment\n",
    "if TRAITS:\n",
    "    evaluator.set_traits(TRAITS, coefficients=COEFFICIENTS)\n",
    "    print(f\"Testing traits: {TRAITS} with coefficients: {COEFFICIENTS}\")\n",
    "else:\n",
    "    evaluator.clear_steering()\n",
    "    print(\"Running baseline (no steering)\")\n",
    "\n",
    "custom_result = evaluator.evaluate(\n",
    "    dataset,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    essay_sets=ESSAY_SETS,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"\\nCustom experiment QWK: {custom_result.qwk:.4f}\")\n",
    "print(f\"Baseline QWK: {baseline_result.qwk:.4f}\")\n",
    "print(f\"Difference: {custom_result.qwk - baseline_result.qwk:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-22",
   "source": [
    "## 8. Batch Evaluation with Multiple Configs\n",
    "\n",
    "Use `run_evaluation` to test multiple configurations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-23",
   "outputs": [],
   "source": [
    "# Define multiple configurations to test\n",
    "configs = [\n",
    "    # Baseline\n",
    "    EvaluationConfig(\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        essay_sets=ESSAY_SETS,\n",
    "        seed=42,\n",
    "    ),\n",
    "    # Trait: thorough\n",
    "    EvaluationConfig(\n",
    "        traits=[\"thorough\"],\n",
    "        coefficients=[-2.0],\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        essay_sets=ESSAY_SETS,\n",
    "        seed=42,\n",
    "    ),\n",
    "    # Trait: pedantic (might be too strict?)\n",
    "    EvaluationConfig(\n",
    "        traits=[\"pedantic\"],\n",
    "        coefficients=[-2.0],\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        essay_sets=ESSAY_SETS,\n",
    "        seed=42,\n",
    "    ),\n",
    "    # Assistant axis positive\n",
    "    EvaluationConfig(\n",
    "        use_assistant_axis=True,\n",
    "        assistant_coefficient=3.0,\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        essay_sets=ESSAY_SETS,\n",
    "        seed=42,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Run all\n",
    "batch_results = run_evaluation(steerer, dataset, configs, verbose=True)\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BATCH EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(compare_results(batch_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-24",
   "source": [
    "## 9. Save and Load Results\n",
    "\n",
    "Save your results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-25",
   "outputs": [],
   "source": [
    "# Save results\n",
    "baseline_result.save(\"../outputs/eval_baseline.json\")\n",
    "print(\"Results saved!\")\n",
    "\n",
    "# Load back\n",
    "# from assistant_axis.evaluation import EvaluationResult\n",
    "# loaded = EvaluationResult.load(\"../outputs/eval_baseline.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-26",
   "source": [
    "## 10. Inspect Individual Predictions\n",
    "\n",
    "Look at specific examples to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-27",
   "outputs": [],
   "source": [
    "# Find examples where predictions differ between configs\n",
    "print(\"Examples where baseline and steered predictions differ:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for baseline_pred, steered_pred in zip(baseline_result.predictions[:20], \n",
    "                                        thorough_result.predictions[:20]):\n",
    "    if baseline_pred['predicted_score'] != steered_pred['predicted_score']:\n",
    "        print(f\"ID: {baseline_pred['id']}\")\n",
    "        print(f\"True Score: {baseline_pred['true_score']}\")\n",
    "        print(f\"Baseline Prediction: {baseline_pred['predicted_score']}\")\n",
    "        print(f\"Steered Prediction: {steered_pred['predicted_score']}\")\n",
    "        print(f\"Answer: {baseline_pred['answer_text'][:100]}...\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-28",
   "source": [
    "## Summary\n",
    "\n",
    "This evaluation framework allows you to:\n",
    "\n",
    "1. **Load datasets**: `ASAPSASDataset(path)`\n",
    "2. **Configure traits**: `evaluator.set_traits([\"trait1\", \"trait2\"], coefficients=[c1, c2])`\n",
    "3. **Use assistant axis**: `evaluator.set_assistant_steering(coefficient)`\n",
    "4. **Run evaluations**: `evaluator.evaluate(dataset, num_samples=N)`\n",
    "5. **Compare results**: `compare_results([result1, result2, ...])`\n",
    "6. **Batch evaluate**: `run_evaluation(steerer, dataset, [config1, config2, ...])`\n",
    "\n",
    "**Key Metrics**:\n",
    "- **QWK** (Quadratic Weighted Kappa): Primary metric, accounts for ordinal nature of scores\n",
    "- **Accuracy**: Exact match rate\n",
    "- **MAE**: Mean absolute error\n",
    "- **Adjacent Agreement**: Predictions within 1 point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
